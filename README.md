# liteLLM
Basic Transformer based multi-head self-attention model
